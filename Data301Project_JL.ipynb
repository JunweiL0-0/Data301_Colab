{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data301Project_JL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data301 Project\n",
        "#### Student: jli328 (Junwei Liang)\n",
        "#### Student Number: 91925811"
      ],
      "metadata": {
        "id": "uLyeuf225YhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background:\n",
        "#### Period: 12-29-2019 - Now"
      ],
      "metadata": {
        "id": "HVa1QKjS_tD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid_start_date = '2019 DEC 12'"
      ],
      "metadata": {
        "id": "yZElcwreFVjS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Libraries"
      ],
      "metadata": {
        "id": "JKw84_7V5n-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWRIDXvb5Pu9"
      },
      "outputs": [],
      "source": [
        "# library and code setup\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# pyspark\n",
        "!pip install -q pyspark\n",
        "# newspaper3k for text analyze\n",
        "!pip install newspaper3k\n",
        "# gdelt for python\n",
        "!pip install gdelt\n",
        "# Test analyze\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "\n",
        "import pyspark, os\n",
        "from pyspark import SparkConf, SparkContext\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yHWPz7-s5x2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Setup and Debug function"
      ],
      "metadata": {
        "id": "umJRD-W75x8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start spark local server\n",
        "import sys, os\n",
        "from operator import add\n",
        "import time\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine\n",
        "try:\n",
        "  conf = SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"1g\")\n",
        "  sc = SparkContext(conf = conf)\n",
        "except ValueError:\n",
        "  #it's ok if the server is already started\n",
        "  pass\n",
        "\n",
        "# A debug function used to print the rdd\n",
        "def dbg(x):\n",
        "  \"\"\" A helper function to print debugging information on RDDs \"\"\"\n",
        "  if isinstance(x, pyspark.RDD):\n",
        "    print([(t[0], list(t[1]) if \n",
        "            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])\n",
        "           if isinstance(t, tuple) else t\n",
        "           for t in x.take(100)])\n",
        "  else:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "dq6hwFrs53wo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove previous files"
      ],
      "metadata": {
        "id": "BodggIZQNVbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r *"
      ],
      "metadata": {
        "id": "af4mfLdtNUla"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch gdelt data using gdeltPyR library"
      ],
      "metadata": {
        "id": "zwu8p8vWEwV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from datetime import date, timedelta\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import gdelt\n",
        "import os\n",
        "\n",
        "# Ignore the future warning\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "# set up gdeltpyr for version 2\n",
        "gd = gdelt.gdelt(version=2)\n",
        "\n",
        "# multiprocess the query\n",
        "e = ProcessPoolExecutor()\n",
        "\n",
        "# generic functions to pull and write data to disk based on date\n",
        "def get_filename(x):\n",
        "  # This will be some things like 20161101(YearMonthDay)\n",
        "  date = x.strftime('%Y%m%d')\n",
        "  return \"{}_gdeltdata.csv\".format(date)\n",
        "\n",
        "# Write retrived data into files\n",
        "def intofile(filename):\n",
        "    try:\n",
        "        if not os.path.exists(\"csvfiles_before\"):\n",
        "           os.mkdir(\"csvfiles_before\")\n",
        "        # If we dont have the file\n",
        "        if not file_exists(filename):\n",
        "          date = filename.split(\"_\")[0]\n",
        "          d = gd.Search(date, table='events', coverage=False) # not updata at 15mins\n",
        "          d.to_csv('csvfiles_before/'+filename, encoding='utf-8', index=False)\n",
        "        return 'csvfiles_before/'+filename\n",
        "    except Exception as e:\n",
        "          print(e)\n",
        "          print(\"Error occurred while retriving the \" + date + \" events tables\")\n",
        "\n",
        "def file_exists(filename):\n",
        "    # Return true if file exist in the storage\n",
        "    return os.path.exists(filename)\n",
        "\n",
        "# pull the data from gdelt into multi files; this may take a long time\n",
        "# I set the data for one month, so that this notebook can be run in a reasonable time (about 30 mins)\n",
        "files_before_covid = [get_filename(x) for x in pd.date_range('2019 Nov 12', covid_start_date)]\n",
        "# pull the data base on the date and write them into files.\n",
        "success_written_files = list(e.map(intofile, files_before_covid))"
      ],
      "metadata": {
        "id": "2FyGfKnLFgqy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiprocess the query\n",
        "e = ProcessPoolExecutor()\n",
        "\n",
        "# generic functions to pull and write data to disk based on date\n",
        "def get_filename(x):\n",
        "  # This will be some things like 20161101(YearMonthDay)\n",
        "  date = x.strftime('%Y%m%d')\n",
        "  return \"{}_gdeltdata.csv\".format(date)\n",
        "\n",
        "# Write retrived data into files\n",
        "def intofile(filename):\n",
        "    try:\n",
        "        if not os.path.exists(\"csvfiles_after\"):\n",
        "           os.mkdir(\"csvfiles_after\")\n",
        "        # If we dont have the file\n",
        "        if not file_exists(filename):\n",
        "          date = filename.split(\"_\")[0]\n",
        "          d = gd.Search(date, table='events', coverage=False) # not updata at 15mins\n",
        "          d.to_csv('csvfiles_after/'+filename, encoding='utf-8', index=False)\n",
        "        return 'csvfiles_after/'+filename\n",
        "    except Exception as e:\n",
        "          print(e)\n",
        "          print(\"Error occurred while retriving the \" + date + \" events tables\")\n",
        "\n",
        "def file_exists(filename):\n",
        "    # Return true if file exist in the storage\n",
        "    return os.path.exists(filename)\n",
        "\n",
        "# pull the data from gdelt into multi files; this may take a long time\n",
        "# I set the data for one month, so that this notebook can be run in a reasonable time (about 30 mins)\n",
        "files_after_covid = [get_filename(x) for x in pd.date_range(covid_start_date,'2020 Jan 12')]\n",
        "# pull the data base on the date and write them into files.\n",
        "success_written_files = list(e.map(intofile, files_after_covid))"
      ],
      "metadata": {
        "id": "z0tn00OQsBSG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read downloaded files into RDDs"
      ],
      "metadata": {
        "id": "suWlMOzbHU1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "# Get the data\n",
        "data_before_covid = (sqlContext.read\n",
        "                               .option(\"header\", \"true\")\n",
        "                               .csv(\"csvfiles_before\"))\n",
        "data_after_covid = (sqlContext.read\n",
        "                              .option(\"header\", \"true\")\n",
        "                              .csv(\"csvfiles_after\"))"
      ],
      "metadata": {
        "id": "LRU6F9F7HT_0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Category events into different types"
      ],
      "metadata": {
        "id": "w6bjtku4Ailt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually set the event types \n",
        "# Event codes categorised into positive, neutral, and negative events.\n",
        "# Neutral events like: Decline comment, Appeal to yield..\n",
        "neutral_events = ['011', '019', '020', '024', '0241', '0242', '025', '0253', \n",
        "                  '0341', '0342', '0343', '0344', '035', '040', '041', '042',\n",
        "                  '043', '044', '045', '046', '080', '083', '0831', '0832', '0833',\n",
        "                  '0834', '084', '0841', '0842', '090', '091', '092', '093',\n",
        "                  '094', '100', '104', '1041', '1042', '1043', '1044', '105', \n",
        "                  '106', '107', '108', '110', '123', '1231', '1232', '1233', \n",
        "                  '1234', '124', '125', '126', '127', '128', '129', '140', \n",
        "                  '141', '1411', '1412', '1413', '1414', '150', '160', '166', '170']\n",
        "\n",
        "# Negative events like: Make pessimistic comment, Make empathetic comment..\n",
        "negative_events = ['012', '016', '111', '112', '1121', '1122', '1123', '1124',\n",
        "                   '1125', '113', '115', '116', '120', '121', '1211', '1212', \n",
        "                   '122', '1221', '1222', '1223', '1224', '1241', '1242',\n",
        "                   '1243', '1244', '1245', '1246', '130', '131', '1311', \n",
        "                   '1312', '1313', '132', '1321', '1322', '1323', '1324', \n",
        "                   '133', '134', '135', '136', '137', '1381', '138114', \n",
        "                   '1382', '1383', '1384', '1385', '139', '142', '1421', \n",
        "                   '1422', '1423', '1424', '143', '1431', '1432', '1433', \n",
        "                   '1434', '144', '1441', '1442', '1443', '1444', '145', \n",
        "                   '1451', '1452', '1453', '1454', '161', '162', '1621', \n",
        "                   '1622', '1623', '163', '164', '165', '1661', '1662', \n",
        "                   '1663', '171', '1711', '1712', '172', '1721', '1722', \n",
        "                   '1723', '1724', '173', '174', '175', '176', '180', '181', \n",
        "                   '182', '1821', '1822', '1823', '183', '1831', '1832', \n",
        "                   '1833', '1834', '184', '185', '186', '190195', '191', \n",
        "                   '192', '193', '194', '1951', '1952', '196', '200', '201',\n",
        "                   '202', '203', '204', '2041', '2042']\n",
        "\n",
        "# Positive events like: Make optimistic comment, Consider policy option..\n",
        "positive_events = ['013', '014', '015', '017', '018', '021', '0211', '0212',\n",
        "                   '0213', '0214', '022', '022', '023', '0231', '0232', '0233',\n",
        "                   '0234', '0243', '0244', '0251', '0252', '0254', '0255', \n",
        "                   '0256', '026', '027', '028', '030', '031', '0311', '0312',\n",
        "                   '0313', '0314', '032', '032', '033', '0331', '0332', '0333', \n",
        "                   '0334', '034', '0351', '0352', '0353', '0354', '0355', '0356', \n",
        "                   '036', '037', '038', '039', '050', '051', '052', '053', \n",
        "                   '054', '055', '056', '057', '060', '061', '062', '063',\n",
        "                   '064', '070', '071', '072', '073', '074', '075', '081', \n",
        "                   '0811', '0812', '0813', '0814', '082', '085', '086', '0861',\n",
        "                   '0862', '0863', '087', '0871', '0872', '0873', '0874',\n",
        "                   '101', '1011', '1012', '1013', '1014', '102', '103', '1031',\n",
        "                   '1032', '1033', '1034', '1051', '1052', '1053', '1054', \n",
        "                   '1055', '1056', '150', '151', '152', '153', '154', '155']\n",
        "\n",
        "# Take the event code and return the event type it belongs to. If it doesn't match any of them, return 'Unknown'\n",
        "def event_sign(event_code):\n",
        "  return (\"Positive\" if event_code in positive_events else\n",
        "          \"Neutral\" if event_code in neutral_events else\n",
        "          \"Negative\" if event_code in negative_events else \"Unknown\")"
      ],
      "metadata": {
        "id": "zYfjSNJoAKmU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Countries"
      ],
      "metadata": {
        "id": "TdMZ3-InD4RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These will be the ActorlCountryCode in the event table\n",
        "target_countries = ['USA', 'CAN', 'MEX', 'GBR', \n",
        "                    'DEU', 'NZL', 'CHN', 'RUS']"
      ],
      "metadata": {
        "id": "A9kHptUIDs3_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import urllib\n",
        "from urllib.request import Request, urlopen, urlretrieve\n",
        "\n",
        "# Retrieve the url from row\n",
        "def get_urls(country, row):\n",
        "    return (country, row['SOURCEURL'])\n",
        "\n",
        "# Get the web page(html) and write it into files\n",
        "def write_wget(withid):\n",
        "    country = withid[0][0]\n",
        "    url = withid[0][1]\n",
        "    id = withid[1]\n",
        "    # Create 'articles' directory if we don't have one\n",
        "    if not os.path.exists(f'{country}_articles'):\n",
        "        os.mkdir(f'{country}_articles')\n",
        "    s = f'{country}_articles/{str(id)}.html'\n",
        "    dst = os.getcwd() + '/' + s\n",
        "    # If we don't have the article yet\n",
        "    if not os.path.exists(dst):\n",
        "      try:\n",
        "          # Pull the url and write it into destination file\n",
        "          urlretrieve(url, dst)\n",
        "          print(s,\"completed\")\n",
        "      except urllib.error.HTTPError as e: #catch errors and peek some, heaps were 403\n",
        "          if e.code == 403: # 403 blocked so opening as user-agent\n",
        "            try: # Not sure if needed but better safe\n",
        "              req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              web_byte = urlopen(req).read()\n",
        "              webpage = web_byte.decode('utf-8')\n",
        "              f = open(dst, \"a\")\n",
        "              f.write(webpage)\n",
        "              print(s,\"completed\")\n",
        "            except Exception as e: #catch unknown errors, and skip them\n",
        "              print(\"failed due to \", e ,\", skipping \", s)\n",
        "          else:\n",
        "            print(e.code, url, id, s) # attn data301, if you get non 404 errors you can handle them like above with \"if e.code == 403\"\n",
        "      except Exception as e:\n",
        "          print(\"Non http error occured while retrieving \" + url + \" id: \" + id) # Catch all other errors\n",
        "    return id\n",
        "\n",
        "# Return the filename if the file exist, otherwise return None\n",
        "def test_file_exist(x):\n",
        "    if os.path.exists(f'{os.getcwd()}/USA_articles/{x[1]}.html'):\n",
        "      return x\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "countries_allids_before_covid = dict()\n",
        "countries_allids_after_covid = dict()\n",
        "# Retrieve urls from rows\n",
        "for country in target_countries:\n",
        "    all_urls_withids_before_covid = (data_before_covid.rdd\n",
        "                                    .filter(lambda row: row['Actor1CountryCode']==country)\n",
        "                                    .map(lambda row: get_urls(country, row))\n",
        "                                    .zipWithUniqueId())\n",
        "    all_urls_withids_after_covid = (data_after_covid.rdd\n",
        "                                    .filter(lambda row: row['Actor1CountryCode']==country)\n",
        "                                    .map(lambda row: get_urls(country, row))\n",
        "                                    .zipWithUniqueId())\n",
        "    # Create a new executor\n",
        "    e = ProcessPoolExecutor(16)\n",
        "    # Retrieve the url and write it into files\n",
        "    e.map(write_wget, all_urls_withids_before_covid.collect()+all_urls_withids_after_covid.collect())\n",
        "    # Revalidate the files and keeps those files we successfully pulled\n",
        "    countries_allids_before_covid[country] = all_urls_withids_before_covid.map(test_file_exist).filter(lambda x: x is not None)\n",
        "    countries_allids_after_covid[country] = all_urls_withids_after_covid.map(test_file_exist).filter(lambda x: x is not None)"
      ],
      "metadata": {
        "id": "W-xPexsHUc_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate similarity"
      ],
      "metadata": {
        "id": "2eGWc78JC4Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_country_pos_neg_summary(data, target_countries):\n",
        "    # ((Country, EventSign), 1) and filter out None value\n",
        "    country_event_types = (data.rdd\n",
        "                               .map(lambda row: ((row['Actor1CountryCode'], event_sign(row['EventCode'])), 1))\n",
        "                               .filter(lambda x: x[0][0] is not None)) # Remove unexist files\n",
        "    # Sum up all the eventSign for each country\n",
        "    summary = country_event_types.reduceByKey(lambda a, b: a+b)\n",
        "    # Only keeps the countries we are interested\n",
        "    country_summary = summary.filter(lambda countries_eventSign_count: countries_eventSign_count[0][0] in target_countries)\n",
        "    # Return a map {(Country, EventSign), count}\n",
        "    return {countries_eventSign : count for countries_eventSign, count in country_summary.collect()}\n",
        "\n",
        "# Get tone for each country\n",
        "def get_country_tone(data, target_countries):\n",
        "    country_tone = (data.rdd\n",
        "                        .map(lambda row: (row['Actor1CountryCode'], float(row['AvgTone'])))\n",
        "                        .filter(lambda x: x[0] is not None) # Remove unexist files\n",
        "                        .filter(lambda country_tone: country_tone[0] in target_countries)\n",
        "                        .groupByKey()\n",
        "                        .map(lambda x: (x[0], get_average_tone(x[1]))))\n",
        "    return country_tone.collect()\n",
        "\n",
        "# Get the average tone\n",
        "def get_average_tone(data):\n",
        "    counter = 0\n",
        "    for average_tone in data:\n",
        "        counter += average_tone\n",
        "    return counter / len(data)\n",
        "\n",
        "def get_vector(result, country):\n",
        "    # Get the country's count and add it to the array\n",
        "    return np.array([result[(country, 'Positive')],\n",
        "                     result[(country, 'Negative')], \n",
        "                     result[(country, 'Neutral')],\n",
        "                     result[(country, 'Unknown')]])\n",
        "\n",
        "def cos_similarity(v1, v2):\n",
        "    # Calculate the cosin similarity\n",
        "    return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def print_similarity(country, target_countries, before_dict, after_dict):\n",
        "    for c in target_countries:\n",
        "        before_similarity = cos_similarity(before_dict[country], before_dict[c])\n",
        "        after_similarity = cos_similarity(after_dict[country], after_dict[c])\n",
        "        print(country, '->', c, \"Similarity before -> after \", before_similarity, \"->\", after_similarity)\n",
        "        print(country, '->', c, \"Reaction similarity \", after_similarity-before_similarity)\n",
        "\n",
        "# EventSign Positive, Negative, Neutral, Unknown\n",
        "summary_before_covid = get_country_pos_neg_summary(data_before_covid, target_countries)\n",
        "summary_after_covid = get_country_pos_neg_summary(data_after_covid, target_countries)\n",
        "# Countries tone\n",
        "country_tone_before_covid = get_country_tone(data_before_covid, target_countries)\n",
        "country_tone_after_covid = get_country_tone(data_after_covid, target_countries)\n",
        "# Get vectors for each country\n",
        "before = {country : get_vector(summary_before_covid, country) for country in target_countries}\n",
        "after = {country : get_vector(summary_after_covid, country) for country in target_countries}\n",
        "# Average tone\n",
        "average_tone_before = {country : average_tone for country, average_tone in country_tone_before_covid}\n",
        "average_tone_after = {country : average_tone for country, average_tone in country_tone_after_covid}\n",
        "# Print the result\n",
        "for country in target_countries:\n",
        "    print(country)\n",
        "    print(\"Vectors before -> after \")\n",
        "    print(\"         [Positive, Negative, Neutral, Unknown]\")\n",
        "    print(\"Before\", before[country])\n",
        "    print(\"After\", after[country])\n",
        "    print_similarity(country, target_countries, before, after)\n",
        "    print(\"AverageTone before -> after\")\n",
        "    print(\"Before\", average_tone_before[country])\n",
        "    print(\"After\", average_tone_after[country])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "8wRCigbUC_ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords"
      ],
      "metadata": {
        "id": "NIJ7Dac4HO8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Caution: This process will take you about 40mins\n",
        "# all keywords before\n",
        "def output_keywords(htmlfilepath):\n",
        "  try:\n",
        "    country = htmlfilepath.split('_')[0]\n",
        "    id = htmlfilepath.split('/')[1].split('.')[0]\n",
        "    # If the html file doesn't exist\n",
        "    if not os.path.exists(f'{country}_keywords_before'):\n",
        "        os.mkdir(f'{country}_keywords_before')\n",
        "    # If we haven't got the file\n",
        "    if not os.path.exists(f\"{country}_keywords_before/{id}.txt\"):\n",
        "      # Create one directory if we haven't got one.\n",
        "      if not os.path.exists(f'{country}_keywords_before'):\n",
        "          os.mkdir(f'{country}_keywords_before')\n",
        "      filenameurl = \"file://\" + os.getcwd() + '/'+ htmlfilepath\n",
        "      with open(htmlfilepath) as f:\n",
        "          s = f.read()\n",
        "          soup = BeautifulSoup(s, features=\"html.parser\")\n",
        "          # kill all script and style elementsi\n",
        "          for script in soup([\"script\", \"style\"]):\n",
        "              script.extract()\n",
        "          # get text\n",
        "          text = soup.get_text()\n",
        "          # break into lines and remove leading and trailing space on each\n",
        "          lines = (line.strip() for line in text.splitlines())\n",
        "          # break multi-headlines into a line each\n",
        "          chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "          # drop blank lines\n",
        "          text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "      # Write the keywords into file\n",
        "      with open(f\"{country}_keywords_before/{id}.txt\", 'w') as keywords_file:\n",
        "          keywords_file.write(text)\n",
        "      print(country, id)\n",
        "      return f\"{country}_keywords_before/{id}.txt\"\n",
        "  except Exception as e:\n",
        "    print(\"Exception throught while on \", htmlfilepath)\n",
        "    print(\"Exception: \" + str(e))\n",
        "    return \"\"\n",
        "\n",
        "# Return the filepath of the article\n",
        "def get_article_filepath(country, id):\n",
        "    return f'{country}_articles/{str(id)}.html'\n",
        "\n",
        "# keywords_count_before = dict()\n",
        "for country in target_countries:\n",
        "  all_article_filepath_before = (countries_allids_before_covid[country].map(lambda x:x[1])\n",
        "                                  .map(lambda id: get_article_filepath(country, id)))\n",
        "  # Create executors\n",
        "  e = ProcessPoolExecutor(16)\n",
        "  e.map(output_keywords, all_article_filepath_before.collect())\n",
        "  # Wait until all finish\n",
        "  e.shutdown(wait=True)"
      ],
      "metadata": {
        "id": "aFcD8qqg-BrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all keywords after\n",
        "def output_keywords(htmlfilepath):\n",
        "  try:\n",
        "      country = htmlfilepath.split('_')[0]\n",
        "      id = htmlfilepath.split('/')[1].split('.')[0]\n",
        "      if not os.path.exists(f'{country}_keywords_after'):\n",
        "          os.mkdir(f'{country}_keywords_after')\n",
        "      # htmlfilepath = f'{country}_articles/{str(id)}.html'\n",
        "      filenameurl = \"file://\" + os.getcwd() + '/'+ htmlfilepath\n",
        "      if not os.path.exists(f'{country}_keywords_after'):\n",
        "          return \"\"\n",
        "      print(country, id)\n",
        "      with open(htmlfilepath) as f:\n",
        "          s = f.read()\n",
        "          soup = BeautifulSoup(s, features=\"html.parser\")\n",
        "          # kill all script and style elementsi\n",
        "          for script in soup([\"script\", \"style\"]):\n",
        "              script.extract()    # rip it out\n",
        "          # get text\n",
        "          text = soup.get_text()\n",
        "          # break into lines and remove leading and trailing space on each\n",
        "          lines = (line.strip() for line in text.splitlines())\n",
        "          # break multi-headlines into a line each\n",
        "          chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "          # drop blank lines\n",
        "          text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "      if not os.path.exists(f\"{country}_keywords_after/{id}.txt\"):\n",
        "          with open(f\"{country}_keywords_after/{id}.txt\", 'w') as keywords_file:\n",
        "              keywords_file.write(text)\n",
        "      return f\"{country}_keywords_after/{id}.txt\"\n",
        "  except Exception as e:\n",
        "    print(htmlfilepath)\n",
        "    print(\"Exception: \" + str(e))\n",
        "    return \"\"\n",
        "\n",
        "def get_article_filepath(country, id):\n",
        "    return f'{country}_articles/{str(id)}.html'\n",
        "\n",
        "# keywords_count_before = dict()\n",
        "for country in target_countries:\n",
        "    all_article_filepath_after = (countries_allids_after_covid[country].map(lambda x:x[1])\n",
        "                                   .map(lambda id: get_article_filepath(country, id)))\n",
        "    # Create\n",
        "    e = ProcessPoolExecutor(16)\n",
        "    e.map(output_keywords, all_article_filepath_after.collect())\n",
        "    # Wait\n",
        "    e.shutdown(wait=True)"
      ],
      "metadata": {
        "id": "m6rH6Wa1K2GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readAndSplitFileIntoRdd(dirpath):\n",
        "  # load the file into a distributed dataset of lines\n",
        "  file = sc.wholeTextFiles(dirpath)\n",
        "  # split each line into (word, 1) tuples\n",
        "  words = (file\n",
        "           .map(lambda x: x[1])\n",
        "           .flatMap(lambda line: [(word.lower(), 1) for word in line.split(\" \")]))\n",
        "  # reduce by key (the word) the counts and sort descending\n",
        "  result = (words\n",
        "            .reduceByKey(lambda a, b: a + b)\n",
        "            .sortBy(lambda x: x[1], False))\n",
        "  return result"
      ],
      "metadata": {
        "id": "F0uVLDDe4M3d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sc.hadoopConfiguration.set(\"mapreduce.input.fileinputformat.input.dir.recursive\",\"true\")\n",
        "from math import log\n",
        "\n",
        "for country in target_countries:\n",
        "    keywords_before = readAndSplitFileIntoRdd(f'{country}_keywords_before')\n",
        "    hightest_count_before = keywords_before.lookup(\"the\")[0]\n",
        "    TF_before = (keywords_before.map(lambda term : (term[0], term[1] / hightest_count_before)).cache())\n",
        "\n",
        "    keywords_after = readAndSplitFileIntoRdd(f'{country}_keywords_after')\n",
        "    hightest_count_after = keywords_after.lookup(\"the\")[0]\n",
        "    TF_after = (keywords_after.map(lambda term : (term[0], term[1] / hightest_count_after)).cache())\n",
        "\n",
        "    IDFi = (keywords_before\n",
        "            .union(keywords_after)\n",
        "            .groupByKey()\n",
        "            .mapValues(len)\n",
        "            .map(lambda a : (a[0], log(4/a[1], 2)))\n",
        "            .cache())\n",
        "    # Attention. Dont broadcast this if the value is too large\n",
        "    broadcast_IDFi = sc.broadcast(dict(IDFi.collect()))\n",
        "    TF_IDF_before = (TF_before\n",
        "                    .map(lambda term : (term[0], term[1]*broadcast_IDFi.value.get(term[0])))\n",
        "                    .sortBy(lambda term : -term[1])\n",
        "                    .cache())\n",
        "    print(TF_IDF_before.take(100))"
      ],
      "metadata": {
        "id": "8ikaAGDHZq35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save all resources\n",
        "# !zip -o CAN_keywords_after.zip CAN_keywords_after/*\n",
        "# !zip -o CHN_keywords_after.zip CHN_keywords_after/*\n",
        "# !zip -o DEU_keywords_after.zip DEU_keywords_after/*\n",
        "# !zip -o GBR_keywords_after.zip GBR_keywords_after/*\n",
        "# !zip -o MEX_keywords_after.zip MEX_keywords_after/*\n",
        "# !zip -o NZL_keywords_after.zip NZL_keywords_after/*\n",
        "# !zip -o RUS_keywords_after.zip RUS_keywords_after/*\n",
        "# !zip -o USA_keywords_after.zip USA_keywords_after/*\n",
        "# # Save all resources\n",
        "# !zip -o CAN_keywords_before.zip CAN_keywords_before/*\n",
        "# !zip -o CHN_keywords_before.zip CHN_keywords_before/*\n",
        "# !zip -o DEU_keywords_before.zip DEU_keywords_before/*\n",
        "# !zip -o GBR_keywords_before.zip GBR_keywords_before/*\n",
        "# !zip -o MEX_keywords_before.zip MEX_keywords_before/*\n",
        "# !zip -o NZL_keywords_before.zip NZL_keywords_before/*\n",
        "# !zip -o RUS_keywords_before.zip RUS_keywords_before/*\n",
        "# !zip -o USA_keywords_before.zip USA_keywords_before/*\n",
        "# # Save all resources\n",
        "# !zip -o CAN_articles.zip CAN_articles/*\n",
        "# !zip -o CHN_articles.zip CHN_articles/*\n",
        "# !zip -o DEU_articles.zip DEU_articles/*\n",
        "# !zip -o GBR_articles.zip GBR_articles/*\n",
        "# !zip -o MEX_articles.zip MEX_articles/*\n",
        "# !zip -o NZL_articles.zip NZL_articles/*\n",
        "# !zip -o RUS_articles.zip RUS_articles/*\n",
        "# !zip -o USA_articles.zip USA_articles/*\n",
        "# !zip -o csvfiles_after.zip csvfiles_after/*\n",
        "# !zip -o csvfiles_before.zip csvfiles_before/*"
      ],
      "metadata": {
        "id": "YtSOSNsMklzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip CAN_keywords_before.zip\n",
        "# !unzip CAN_keywords_before.zip\n",
        "# !unzip CAN_keywords_after.zip\n",
        "# !unzip CHN_keywords_before.zip\n",
        "# !unzip CHN_keywords_after.zip\n",
        "# !unzip DEU_keywords_before.zip\n",
        "# !unzip DEU_keywords_after.zip\n",
        "# !unzip GBR_keywords_before.zip\n",
        "# !unzip GBR_keywords_after.zip\n",
        "# !unzip MEX_keywords_before.zip\n",
        "# !unzip MEX_keywords_after.zip\n",
        "# !unzip NZL_keywords_before.zip \n",
        "# !unzip NZL_keywords_after.zip\n",
        "# !unzip RUS_keywords_before.zip \n",
        "# !unzip RUS_keywords_after.zip y\n",
        "# # Special unzip\n",
        "# !sudo apt-get install fastjar\n",
        "# !jar xvf USA_keywords_before.zip\n",
        "# !jar xvf USA_keywords_after.zip"
      ],
      "metadata": {
        "id": "alVwo5I3SLDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}